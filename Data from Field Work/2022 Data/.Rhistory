d <- as.numeric(is.na(OtterData$TTD)) # Censoring indicator
M=length(UNQ$Watershed)
nobs=length(OtterData$TTD)
TTDF=vector(length = nobs)
for (i in 1:nobs){
TTDF[i]=which(COVS$Index==OtterData$Index[i])
}
max(TTDF)
#####
#Packages
#####
library(rjags)
library(runjags)
library(mcmcplots)
library(MCMCvis)
## Set model
model_string <-"model
{
# Likelihood
for (m in 1:M){ # Model for occurrence at site level
z[m] ~ dbern(psi[m])
logit(psi[m])=psi.int+beta1*imp[m]+beta2*forest[m]+beta3*swamp[m]+beta4*salt[m]+beta5*fresh[m]+beta6*Strm[m]+betaw[WSHED[m]]
}
for (w in 1:5){
betaw[w]~dnorm(0,psi.tau)
}
for (i in 1:nobs){ # Observation model at observation level
# Weibull model for time to detection ignoring censoring
ttd[i] ~ dweib(shape,lambda[i])
log(lambda[i]) <- log(lambda.int)+alpha1*OBS[i,2]+alpha2*OBS[i,3]+alpha3*OBS[i,4]+alpha4*OBS[i,5]+alpha5*OBS[i,6]+alpha6*fresh[TTDF[i]]
# Model for censoring due to species absence or ttd>Tmax
d[i] ~ dbern(theta[i])
theta[i] <- z[TTDF[i]] * step(ttd[i] - Tmax[i]) + (1 - z[TTDF[i]])
}
# Priors
psi.int ~ dlogis(0,1) # Occupancy intercept
beta1~dlogis(0,1)
beta2~dlogis(0,1)
beta3~dlogis(0,1)
beta4~dlogis(0,1)
beta5~dlogis(0,1)
beta6~dlogis(0,1)
psi.tau~dgamma(1,1)
lambda.int ~ dgamma(.001,.001)
alpha1~dnorm(0, 0.3)
alpha2~dnorm(0, 0.3)
alpha3~dnorm(0, 0.3)
alpha4~dnorm(0, 0.3)
alpha5~dnorm(0, 0.3)
alpha6~dnorm(0,0.3)
shape~dgamma(1,1)
psim=mean(psi)
}
"
data <- list(M=M,
ttd = OtterData$TTD,
TTDF=TTDF,
d=d,nobs=nobs,
Tmax=OtterData$YMax,
forest=COVS$forest,
imp=COVS$imp,
swamp=COVS$swamp,
salt=COVS$salt,
fresh=COVS$fresh,
WSHED=as.numeric(wshed),
OBS=OBS,
Strm=COVS$Strm2plus)
zst <- rep(1, data$M)
ttdst <-rep(data$Tmax+1)
ttdst[data$d == 0] <- NA
inits=function(){list(z =zst, ttd = ttdst,
psi.int=runif(1),
lambda.int=runif(1),
alpha1=rnorm(1),
alpha2=rnorm(1),
alpha3=rnorm(1),
alpha4=rnorm(1),
alpha5=rnorm(1),
alpha6=rnorm(1),
beta1=rnorm(1),
beta2=rnorm(1),
beta3=rnorm(1),
beta4=rnorm(1),
beta5=rnorm(1),
beta6=rnorm(1),
betaw=rnorm(5),
psi.tau=runif(1))}
Otter.TTD=run.jags(model=model_string,
monitor= c("psim","psi.int","alpha1","alpha2",
"alpha3","alpha4","alpha5","alpha6",
"beta1","beta2","beta3",
"beta4","beta5","beta6","betaw","psi.tau",
"lambda.int",
"shape"),
burnin=10000, sample=20000,
data=data, n.chains=4, method="rjags", inits=inits)
View(OtterData)
180*4
View(TTDFrame21)
rm(list=ls())
setwd("C:/Users/John Crockett/Documents/URI")
OccCovs21=read.csv(file="SiteVaryingCovs21.csv")
TTDFrame21=read.csv(file="TTD21.csv")
OccCovs22=read.csv(file="SiteVaryingCovs22.csv")
TTDFrame22=read.csv(file="TTD22.csv")
View(TTDFrame21)
length(TTDFrame21$TTD[TTDFrame21$Spp="otter"])
length(TTDFrame21$TTD[TTDFrame21$Spp=="otter"])
length(TTDFrame22$TTD[TTDFrame22$Spp=="otter"])
TTDFrame21$TTD[TTDFrame21$Spp!="otter"]=NA
TTDFrame21=TTDFrame21[,c(2:4,6,8,7)]
TTDFrame21=TTDFrame21[!duplicated(TTDFrame21[,-5]),]
TTDFrame22$TTD[TTDFrame22$Spp!="otter"]=NA
TTDFrame22=TTDFrame22[,c(2:4,6:8)]
TTDFrame22=TTDFrame22[!duplicated(TTDFrame22[,-5]),]
colnames(TTDFrame21)=colnames(TTDFrame22)
#Combining Years
OtterData=rbind(TTDFrame21,TTDFrame22)
min(OtterData$TTD,na.rm=T)
OtterData$TTD[OtterData$TTD<1]=0.1
min(OtterData$YMax)
mean(OtterData$YMax,na.rm=T)
OtterData$YMax[is.na(OtterData$YMax)]=mean(OtterData$YMax,na.rm=T)
which(!is.na(OtterData[OtterData$TTD>OtterData$YMax,]))
which(OtterData[OtterData$TTD>OtterData$YMax,])
OtterData[439]
which(!is.na(OtterData$Site.ID[OtterData$TTD>OtterData$YMax,]))
which(!is.na(OtterData$Site.ID[OtterData$TTD>OtterData$YMax]))
View(OtterData)
OtterData$Site.ID[OtterData$TTD>OtterData$YMax])
OtterData$Site.ID[OtterData$TTD>OtterData$YMax]
OtterData[OtterData$Site.ID==176,]
OtterData$YMax[OtterData$TTD>OtterData$YMax]=OtterData$TTD
mean(OtterData$YMax,na.rm=T)
OtterData$YMax[is.na(OtterData$TTD)==F&OtterData$TTD>OtterData$YMax]=OtterData$TTD
OtterData[OtterData$Site.ID==176&OtterData$Day==24]
OtterData[OtterData$Site.ID==176&OtterData$Day==24,]
OtterData$TTD[OtterData$Site.ID==176&OtterData$Day==24]
OtterData$YMax[OtterData$Site.ID==176&OtterData$Day==24]
OtterData$YMax[OtterData$Site.ID==176&OtterData$Day==24]=c(20,29)
#####
#Harmonizing Covariates
#####
colnames(OccCovs21)
OccCovs21=OccCovs21[,c(2,3,4,7,8,11:13)]
colnames(OccCovs22)
OccCovs22=OccCovs22[,c(2:6,9:11)]
colnames(OccCovs21)[1:8]=colnames(OccCovs22)[1:8]=c("Site","Watershed",
colnames(OccCovs22)[3:8])
which(OccCovs22$Site==108)
OccCovs22$Watershed[OccCovs22$Site==108|OccCovs22$Site==162]="Narragansett Bay"
COVS=rbind(OccCovs21,OccCovs22)
#Making sure we get sites right
COVS$Watershed[which(COVS$Watershed=="BIS")]="Block Island Sound"
UNQ=unique(OtterData[,1:2])
COVS$Index=NA
OtterData$Index=NA
for (i in 1:length(UNQ$Watershed)){
OtterData$Index[OtterData$Watershed==UNQ$Watershed[i]&
OtterData$Site.ID==UNQ$Site.ID[i]]=i
COVS$Index[COVS$Watershed==UNQ$Watershed[i]&
COVS$Site==UNQ$Site.ID[i]]=i
}
#####
#Standardizing Covariates
#####
meanimp=mean(COVS$Impervious)
sdimp=sqrt(var(COVS$Impervious))
COVS$imp=(COVS$Impervious-meanimp)/sdimp
meanforest=mean(COVS$Forest)
sdforest=sqrt(var(COVS$Forest))
COVS$forest=(COVS$Forest-meanforest)/sdforest
meansalt=mean(COVS$Salt)
sdsalt=sqrt(var(COVS$Salt))
COVS$salt=(COVS$Salt-meansalt)/sdsalt
meanswamp=mean(COVS$Swamp)
sdswamp=sqrt(var(COVS$Swamp))
COVS$swamp=(COVS$Swamp-meanswamp)/sdswamp
meanfresh=mean(COVS$FreshWater)
sdfresh=sqrt(var(COVS$FreshWater))
COVS$fresh=(COVS$FreshWater-meanfresh)/sdfresh
COVS$Strm2plus=0
COVS$Strm2plus[COVS$StrmOrder>1]=1
#####
#Making the Effect Coding for Watershed and Observer
#####
wshed=as.factor(COVS$Watershed)
levels(wshed)
WSHED=model.matrix(~wshed,contrasts = list(wshed = contr.sum))
colnames(WSHED)=c("Int","BIS","Narr","Pawc","Pawt")
unique(OtterData$Observer)
OtterData$Observer[OtterData$Observer=="Emerson Paton"|
OtterData$Observer=="Emma Paton"|
OtterData$Observer=="Charlie Brown"|
OtterData$Observer=="Richard Mercer"]="Other Observer"
obs=as.factor(OtterData$Observer)
levels(obs)
OBS=model.matrix(~obs,contrasts = list(obs = contr.sum))
colnames(OBS)=c("Int",levels(obs)[1:5])
#####
#Bits and bobs to give the model
#####
d <- as.numeric(is.na(OtterData$TTD)) # Censoring indicator
M=length(UNQ$Watershed)
nobs=length(OtterData$TTD)
TTDF=vector(length = nobs)
for (i in 1:nobs){
TTDF[i]=which(COVS$Index==OtterData$Index[i])
}
max(TTDF)
#####
#Packages
#####
library(rjags)
library(runjags)
library(mcmcplots)
library(MCMCvis)
## Set model
model_string <-"model
{
# Likelihood
for (m in 1:M){ # Model for occurrence at site level
z[m] ~ dbern(psi[m])
logit(psi[m])=psi.int+beta1*imp[m]+beta2*forest[m]+beta3*swamp[m]+beta4*salt[m]+beta5*fresh[m]+beta6*Strm[m]+betaw[WSHED[m]]
}
for (w in 1:5){
betaw[w]~dnorm(0,psi.tau)
}
for (i in 1:nobs){ # Observation model at observation level
# Weibull model for time to detection ignoring censoring
ttd[i] ~ dweib(shape,lambda[i])
log(lambda[i]) <- log(lambda.int)+alpha1*OBS[i,2]+alpha2*OBS[i,3]+alpha3*OBS[i,4]+alpha4*OBS[i,5]+alpha5*OBS[i,6]+alpha6*fresh[TTDF[i]]
# Model for censoring due to species absence or ttd>Tmax
d[i] ~ dbern(theta[i])
theta[i] <- z[TTDF[i]] * step(ttd[i] - Tmax[i]) + (1 - z[TTDF[i]])
}
# Priors
psi.int ~ dlogis(0,1) # Occupancy intercept
beta1~dlogis(0,1)
beta2~dlogis(0,1)
beta3~dlogis(0,1)
beta4~dlogis(0,1)
beta5~dlogis(0,1)
beta6~dlogis(0,1)
psi.tau~dgamma(1,1)
lambda.int ~ dgamma(.001,.001)
alpha1~dnorm(0, 0.3)
alpha2~dnorm(0, 0.3)
alpha3~dnorm(0, 0.3)
alpha4~dnorm(0, 0.3)
alpha5~dnorm(0, 0.3)
alpha6~dnorm(0,0.3)
shape~dgamma(1,1)
psim=mean(psi)
}
"
data <- list(M=M,
ttd = OtterData$TTD,
TTDF=TTDF,
d=d,nobs=nobs,
Tmax=OtterData$YMax,
forest=COVS$forest,
imp=COVS$imp,
swamp=COVS$swamp,
salt=COVS$salt,
fresh=COVS$fresh,
WSHED=as.numeric(wshed),
OBS=OBS,
Strm=COVS$Strm2plus)
zst <- rep(1, data$M)
ttdst <-rep(data$Tmax+1)
ttdst[data$d == 0] <- NA
inits=function(){list(z =zst, ttd = ttdst,
psi.int=runif(1),
lambda.int=runif(1),
alpha1=rnorm(1),
alpha2=rnorm(1),
alpha3=rnorm(1),
alpha4=rnorm(1),
alpha5=rnorm(1),
alpha6=rnorm(1),
beta1=rnorm(1),
beta2=rnorm(1),
beta3=rnorm(1),
beta4=rnorm(1),
beta5=rnorm(1),
beta6=rnorm(1),
betaw=rnorm(5),
psi.tau=runif(1))}
Otter.TTD=run.jags(model=model_string,
monitor= c("psim","psi.int","alpha1","alpha2",
"alpha3","alpha4","alpha5","alpha6",
"beta1","beta2","beta3",
"beta4","beta5","beta6","betaw","psi.tau",
"lambda.int",
"shape"),
burnin=10000, sample=20000,
data=data, n.chains=4, method="rjags", inits=inits)
length(OtterData$TTD>0)
length(!is.na(OtterData$TTD>0))
length(!is.na(OtterData$TTD))
length(which(!is.na(OtterData$TTD)))
COVS[TTDF[which(!is.na(OtterData$TTD))],]
COVS[TTDF[which(!is.na(OtterData$TTD))],c(1,2,10:15)]
setwd("C:/Users/John Crockett/Documents/URI")
load(file="BeaverRandom.Rdata")
load(file="BeaverRandom27Feb.Rdata")
load(file = "BeaverTTD.Rdata")
library(runjags)
#combine chains
mcmc=data.frame(combine.mcmc(Beaver.TTD))
colnames(mcmc)
lambda=mcmc$lambda.int
v=mcmc$shape
#To use R's function, we need to convert to an alternative parameterization
#See wikipedia: https://en.wikipedia.org/wiki/Weibull_distribution
#Look under "First Alternative", under "Alternative Parameterizations"
shape=v
scale=lambda^(-shape)
#plot observed data
hist(data$ttd,freq=FALSE)
#Add weibull density, using medians of posterior parameters
curve(dweibull(x,shape=median(shape),scale = median(scale)),lwd=2,col=2,add=T)
#Derive the median ttd using wikipedia notation
median.ttd=scale*(log(2))^(1/shape)
hist(median.ttd)
#Derive the mean ttd
mean.ttd=scale*gamma(1+1/shape)
hist(mean.ttd)
#Mean doesn't make as much sense as the mean
#################################
#Alternative idea in plotting.
#See Kery and Royle book page 621 (section 10.13). Seee code and how to plot probability of detection
#in figure 10.16
duration=1:80
prob=1-exp(-exp(log(mean(mcmc$lambda.int)))*duration)
plot(duration,prob,type="l",lwd=3,xlab="Survey Duration",ylab="Probability of Detection")
#The shape parameter is less than one indicating that the hazard detection declines with with increasing observation time.
#This suggests that you either usually start in a good place or look at the good places first, but then as time goes
#on there is increasing pessimism about finding a detection. See text in book
hist(mcmc$shape)
#################
lambda_f=matrix(nrow = 40000,ncol=184)
for(i in 1:length(data$fresh)){
lambda_f[,i]=exp(log(mcmc$lambda.int)+mcmc$alpha6*data$fresh[i])
}
#################
lambda_f=matrix(nrow = 80000,ncol=184)
for(i in 1:length(data$fresh)){
lambda_f[,i]=exp(log(mcmc$lambda.int)+mcmc$alpha6*data$fresh[i])
}
scale_f=matrix(nrow = 80000,ncol=184)
for(i in 1:length(data$fresh)){
scale_f[,i]=lambda_f[,i]^(-shape)
}
median_scale_f=apply(scale_f,2,median)
hist(data$ttd,freq=FALSE)
x=seq(from=0.01,to=60,by=.01)
dweib_out=matrix(ncol=184,nrow=6000)
for(i in 1:184){
dweib_out[,i]=dweibull(x,shape = median(shape),scale = median_scale_f[i])
}
library(viridis)
pal=viridis(53)
fresh2pal=round(data$fresh*10)+6
hist(data$ttd,freq=FALSE)
for(i in 1:184){
curve(dweibull(x,shape=median(shape),scale = median_scale_f[i]),
col=pal[round(data$fresh[i]*10)+6],add=T)
}
plot(1:184,data$fresh,col=pal[fresh2pal],pch=19)
plot(data$fresh,col=pal[fresh2pal])
#png(file="Feb2MuskTTDbyFresh.png",res=1200, width=12, height=8,units="in")
hist(data$ttd,freq=FALSE)
for(i in 1:184){
curve(dweibull(x,shape=median(shape),scale = median_scale_f[i]),
col=pal[round(data$fresh[i]*10)+6],add=T)
}
#####
lambda_o=matrix(nrow = 40000,ncol=6)
#####
lambda_o=matrix(nrow = 80000,ncol=6)
for(i in 1:data$nobs){
lambda_o[,1]=exp(log(mcmc$lambda.int))
lambda_o[,2]=exp(log(mcmc$lambda.int)+mcmc$alpha1)
lambda_o[,3]=exp(log(mcmc$lambda.int)+mcmc$alpha2)
lambda_o[,4]=exp(log(mcmc$lambda.int)+mcmc$alpha3)
lambda_o[,5]=exp(log(mcmc$lambda.int)+mcmc$alpha4)
lambda_o[,6]=exp(log(mcmc$lambda.int)+mcmc$alpha5)
}
scale_o=matrix(nrow = 80000,ncol=6)
for(i in 1:6){
scale_o[,i]=lambda_o[,i]^(-shape)
}
median_scale_o=apply(scale_o,2,median)
hist(data$ttd,freq=FALSE)
x=seq(from=0.01,to=60,by=.01)
dweib_out=matrix(ncol=6,nrow=6000)
for(i in 1:6){
dweib_out[,i]=dweibull(x,shape = median(shape),scale = median_scale_o[i])
}
library(viridis)
library(RColorBrewer)
pal=brewer.pal(n=6,name = "Set1")
obs=data$OBS
obs.list=dimnames(obs)[[2]]
hist(data$ttd,freq=FALSE,main = "Predicted time to detection by oberver",
xlab="Time to detection (minutes)")
for(i in 1:6){
curve(dweibull(x,shape=median(shape),scale = median_scale_o[i]),
col=pal[i],lwd=2,add=T)
}
legend("topright",legend = c("Other Observers",obs.list[2:6]),fill = pal)
duration=1:80
prob_o=matrix(nrow=80,ncol=6)
for(i in 1:6){
prob_o[,1]=1-exp(-exp(log(median(mcmc$lambda.int)))*duration)
prob_o[,2]=1-exp(-exp(log(median(mcmc$lambda.int))+median(mcmc$alpha1))*duration)
prob_o[,3]=1-exp(-exp(log(median(mcmc$lambda.int))+median(mcmc$alpha2))*duration)
prob_o[,4]=1-exp(-exp(log(median(mcmc$lambda.int))+median(mcmc$alpha3))*duration)
prob_o[,5]=1-exp(-exp(log(median(mcmc$lambda.int))+median(mcmc$alpha4))*duration)
prob_o[,6]=1-exp(-exp(log(median(mcmc$lambda.int))+median(mcmc$alpha5))*duration)
}
prob_o_lci=prob_o_uci=matrix(nrow=80,ncol=6)
for(i in 1:6){
prob_o_lci[,1]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .025)))*duration)
prob_o_lci[,2]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .025))+quantile(mcmc$alpha1,probs = .025))*duration)
prob_o_lci[,3]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .025))+quantile(mcmc$alpha2,probs = .025))*duration)
prob_o_lci[,4]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .025))+quantile(mcmc$alpha3,probs = .025))*duration)
prob_o_lci[,5]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .025))+quantile(mcmc$alpha4,probs = .025))*duration)
prob_o_lci[,6]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .025))+quantile(mcmc$alpha5,probs = .025))*duration)
}
for(i in 1:6){
prob_o_uci[,1]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .975)))*duration)
prob_o_uci[,2]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .975))+quantile(mcmc$alpha1,probs = .975))*duration)
prob_o_uci[,3]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .975))+quantile(mcmc$alpha2,probs = .975))*duration)
prob_o_uci[,4]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .975))+quantile(mcmc$alpha3,probs = .975))*duration)
prob_o_uci[,5]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .975))+quantile(mcmc$alpha4,probs = .975))*duration)
prob_o_uci[,6]=1-exp(-exp(log(quantile(mcmc$lambda.int,probs = .975))+quantile(mcmc$alpha5,probs = .975))*duration)
}
plot(duration,prob_o[,1],type="l",col=pal[1],ylim=c(0,1),bty="L",
lwd=3,xlab="Survey Duration (minutes)",ylab="Probability of Detection")
polygon(x=c(duration,rev(duration)),y=c(prob_o_lci[,1],rev(prob_o_uci[,1])),
border=NA,col=adjustcolor(col = pal[1],alpha.f = 0.1))
for(i in 2:6){
lines(duration,prob_o[,i],type="l",col=pal[i],lwd=3)
polygon(x=c(duration,rev(duration)),y=c(prob_o_lci[,i],rev(prob_o_uci[,i])),
border=NA,col=adjustcolor(col = pal[i],alpha.f = 0.2))
}
legend("top",inset=c(0,-0.2),xpd=T,ncol=3,bty="n",
legend = c("Other Observers",obs.list[2:6]),fill = pal)
plot(duration,prob_o[,1],type="l",col=pal[1],ylim=c(0,1),bty="L",
lwd=3,xlab="Survey Duration (minutes)",ylab="Probability of Detection")
for(i in 2:6){
lines(duration,prob_o[,i],type="l",col=pal[i],lwd=3)
}
legend("bottom",inset=c(0,-0.2),xpd=T,ncol=3,bty="n",
legend = c("Other Observers",obs.list[2:6]),fill = pal)
legend("bottom",inset=c(0,-0),xpd=T,ncol=3,bty="n",
legend = c("Other Observers",obs.list[2:6]),fill = pal)
plot(duration,prob_o[,1],type="l",col=pal[1],ylim=c(0,1),bty="L",
lwd=3,xlab="Survey Duration (minutes)",ylab="Probability of Detection")
for(i in 2:6){
lines(duration,prob_o[,i],type="l",col=pal[i],lwd=3)
}
legend("bottom",inset=c(0,-0),xpd=T,ncol=3,bty="n",
legend = c("Other Observers",obs.list[2:6]),fill = pal)
rm(list=ls())
setwd("C:/Users/John Crockett/Documents/URI")
OccCovs21=read.csv(file="SiteVaryingCovs21.csv")
TTDFrame21=read.csv(file="TTD21.csv")
OccCovs22=read.csv(file="SiteVaryingCovs22.csv")
TTDFrame22=read.csv(file="TTD22.csv")
length(unique(TTDFrame21$Site.ID))
length(unique(TTDFrame22$Site.ID))
View(TTDFrame21)
rm(list=ls())
setwd("C:/Users/John Crockett/Documents/URI")
OccCovs21=read.csv(file="SiteVaryingCovs21.csv")
TTDFrame21=read.csv(file="TTD21.csv")
OccCovs22=read.csv(file="SiteVaryingCovs22.csv")
TTDFrame22=read.csv(file="TTD22.csv")
View(TTDFrame21)
View(OccCovs22)
library(plotKML)
library(dplyr)
library(lubridate)
library(readxl)
library(data.table)
getwd()
###Clearing all leftover vars
rm(list=ls())
###Setting working directory so R knows to look at this season's data
#setwd("C:/Users/John Crockett/Documents/URI/Data from Field Work/Summer 2021")
setwd("C:/Users/John Crockett/Documents/URI/Data from Field Work/2022 Data")
###Importing the field data and getting R to recognize the time and date as such
#FieldData=data.frame(read_xlsx("Summer 2021 All watersheds.xlsx"))
FieldData=data.frame(read_xlsx("2022 Field Data_withSummer.xlsx"))
View(FieldData)
FieldData$Newtime=ymd_hms(paste0(FieldData$Date,substring(FieldData$Time,first = 12)))
View(FieldData)
library(plotKML)
library(dplyr)
library(lubridate)
getwd()
###Clearing all leftover vars
rm(list=ls())
##Pointing R towards the right folder. Form here on in you could run this##
##on a loop but I didn't want to bother. Something like for (z in 1:5){  ##
##  all the stuff below but using paste0 and assign to get GPS[z] to be  ##
##  legible}
setwd("C:/Users/John Crockett/Documents/URI/Data from Field Work/2022 Data/Tracks/GPS2")
